{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68eda55c",
   "metadata": {},
   "source": [
    "# Images\n",
    "\n",
    "## Introduction to Digital Images\n",
    "\n",
    "Digital images are stored as a grid (matrix) of colored dots called **pixels**. Each pixel typically has three primary color values:\n",
    "\n",
    "- **Red**\n",
    "- **Green**\n",
    "- **Blue**\n",
    "\n",
    "Hence, in code, an RGB image can be considered a 3D array (list) with dimensions corresponding to `(height, width, channels)`. For instance, a 640×480 image has 640 pixels in width (horizontal) and 480 in height (vertical), each pixel containing an `(R, G, B)` triplet.\n",
    "\n",
    "Sometimes, there's an **Alpha** channel (RGBA) representing transparency. We’ll focus on standard RGB for simplicity.\n",
    "\n",
    "In Python, the **Pillow** library (PIL) is a common tool for reading and writing these images, while libraries like [**diffusers**](https://github.com/huggingface/diffusers) offer utility functions (like `load_image`) for convenience.\n",
    "\n",
    "We’ll explore both **Pillow**, **numpy** and **diffusers** for:\n",
    "\n",
    "1. Loading images\n",
    "2. Manipulating images\n",
    "3. Creating simple animations (MP4s)\n",
    "\n",
    "Let’s start by installing and importing the necessary libraries.\n",
    "\n",
    "We will install all necessary libraries in one step using **pip** like discussed in previous chapters:\n",
    "\n",
    "`pip install pillow numpy diffusers tqdm`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc2a84c",
   "metadata": {},
   "source": [
    "## Loading Images\n",
    "\n",
    "There are multiple ways in different libraries to load images stored locally on our device. The most common way is to load it using Pillow (a fort of PIL). See [readthedocs.io](https://pillow.readthedocs.io/en/stable/handbook/tutorial.html) and [automate the boring stuff](https://automatetheboringstuff.com/chapter17/) for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44079481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image # Import the Image class\n",
    "\n",
    "img = Image.open(\"sample.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c079cbe",
   "metadata": {},
   "source": [
    "We can inspect the loaded image like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc24b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: (512, 512)\n",
      "mode: RGB\n",
      "format: JPEG\n"
     ]
    }
   ],
   "source": [
    "print('size:', img.size)\n",
    "print('mode:', img.mode)\n",
    "print('format:', img.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01724bf",
   "metadata": {},
   "source": [
    "To display an image, we can use the show method, which will open the image in the default image-viewer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de2af579",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d979907",
   "metadata": {},
   "source": [
    "### Loading from the Web\n",
    "\n",
    "`Image.open` will only work with local files and return a `PIL`. The `diffusers` library provides a helper function to load images from the web (via ursl) or from disk (using paths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44712b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "\n",
    "img = load_image(\"https://thispersondoesnotexist.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8990bd",
   "metadata": {},
   "source": [
    "The resulting image will be in the `PIL` format aswell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24080c",
   "metadata": {},
   "source": [
    "### Saving images\n",
    "\n",
    "Images loaded in the `PIL` format can be saved locally with a call to `save` which takes a path as the parameter. If no format is provided it will be chosen according to the file-extension in the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a16f0dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "\n",
    "img = load_image(\"https://thispersondoesnotexist.com/\")\n",
    "\n",
    "img.save(\"face1.png\", format=\"PNG\") # Explicit PNG\n",
    "\n",
    "img.save(\"face2.png\") # Implicit PNG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f307cc8",
   "metadata": {},
   "source": [
    "## Manipulating Images\n",
    "\n",
    "Pillow provides many methods for image manipulation. Here are some common ones:\n",
    "\n",
    "1. **Rotate**: `image.rotate(angle, expand=True)`.\n",
    "2. **Resize (Scale)**: `image.resize((new_width, new_height))`.\n",
    "3. **Blur**: using `ImageFilter.BLUR` or other filters from `ImageFilter`.\n",
    "4. **Enhance Contrast**: using `ImageEnhance.Contrast(image)`.\n",
    "5. **Composite**: combine multiple images using `Image.composite`.\n",
    "\n",
    "We'll demonstrate these on an example image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e87d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "\n",
    "img = load_image(\"https://thispersondoesnotexist.com/\")\n",
    "\n",
    "# 1. Rotate\n",
    "rotated_image = img.rotate(45, expand=True)\n",
    "rotated_image.save(\"img/rotated.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de091d",
   "metadata": {},
   "source": [
    "![rotated](img/rotated.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e0c8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Scale\n",
    "width, height = img.size\n",
    "half_sized_image = img.resize((width // 2, height // 2))\n",
    "half_sized_image = img.save(\"img/scaled.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf9470",
   "metadata": {},
   "source": [
    "![rotated](img/scaled.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4410332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Blur\n",
    "from PIL import ImageFilter\n",
    "blurred_image = img.filter(ImageFilter.BLUR)\n",
    "blurred_image.save(\"img/blurred.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a382ff7",
   "metadata": {},
   "source": [
    "![rotated](img/blurred.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f66d1c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Enhance Contrast\n",
    "from PIL import ImageEnhance\n",
    "enhancer = ImageEnhance.Contrast(img)\n",
    "contrast_image = enhancer.enhance(2.0)  # Increase contrast\n",
    "contrast_image.save(\"img/enhanced.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85507790",
   "metadata": {},
   "source": [
    "![rotated](img/enhanced.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f5164a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Composite\n",
    "# For demonstration, we composite the original and blurred image using a gradient mask.\n",
    "mask = Image.linear_gradient(\"L\").resize(img.size)\n",
    "img_rgba = img.convert(\"RGBA\")\n",
    "blurred_rgba = blurred_image.convert(\"RGBA\")\n",
    "composited_img = Image.composite(img_rgba, blurred_rgba, mask)\n",
    "composited_img.save(\"img/composited.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c300b5e",
   "metadata": {},
   "source": [
    "![composited](img/composited.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b86ddb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Crop\n",
    "cropped = img.crop((0, 0, 256, 50))\n",
    "cropped.save(\"img/cropped.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c21c76a",
   "metadata": {},
   "source": [
    "![cropped](img/cropped.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248bc71e",
   "metadata": {},
   "source": [
    "### Loading Images as numpy\n",
    "\n",
    "We can also use numpy to read an image direclty as a three-dimensional list of color values. for that we can convert any `PIL` image to `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a90a710b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions: 3\n",
      "shape: (1024, 1024, 3)\n",
      "size 3145728\n"
     ]
    }
   ],
   "source": [
    "from diffusers.utils import load_image\n",
    "import numpy as np # numpy is usually renamed to np\n",
    "\n",
    "# Load as PIL\n",
    "img_pil = load_image(\"https://thispersondoesnotexist.com/\")\n",
    "\n",
    "\n",
    "# convert to numpy\n",
    "img_np = np.asarray(img_pil)\n",
    "print('dimensions:', img_np.ndim)\n",
    "print('shape:', img_np.shape)\n",
    "print('size', img_np.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab290296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 77  80  63]\n",
      "  [ 50  55  33]\n",
      "  [ 41  50  23]\n",
      "  ...\n",
      "  [ 26  39  21]\n",
      "  [ 23  35  21]\n",
      "  [ 33  43  32]]\n",
      "\n",
      " [[ 52  58  44]\n",
      "  [ 35  42  26]\n",
      "  [ 34  41  23]\n",
      "  ...\n",
      "  [ 26  39  22]\n",
      "  [ 26  38  24]\n",
      "  [ 22  35  25]]\n",
      "\n",
      " [[ 39  47  36]\n",
      "  [ 31  39  28]\n",
      "  [ 34  42  31]\n",
      "  ...\n",
      "  [ 24  35  19]\n",
      "  [ 21  35  20]\n",
      "  [ 17  33  20]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[206 194 182]\n",
      "  [199 187 173]\n",
      "  [183 174 157]\n",
      "  ...\n",
      "  [197 180 164]\n",
      "  [199 181 169]\n",
      "  [207 187 180]]\n",
      "\n",
      " [[205 190 187]\n",
      "  [205 191 182]\n",
      "  [186 172 161]\n",
      "  ...\n",
      "  [196 179 169]\n",
      "  [211 192 185]\n",
      "  [206 183 177]]\n",
      "\n",
      " [[198 179 183]\n",
      "  [207 187 186]\n",
      "  [188 169 162]\n",
      "  ...\n",
      "  [203 185 181]\n",
      "  [219 200 196]\n",
      "  [201 176 172]]]\n"
     ]
    }
   ],
   "source": [
    "print(img_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14238ba4",
   "metadata": {},
   "source": [
    "We can now manipulate pixel values directly using mathematical functions, but we can also use generative artificial intelligence to write the functions for us. For that we will just need to formulate our prompt accordingly.\n",
    "\n",
    "As a base prompt we can use the following:\n",
    "\n",
    "`Give me a python function that takes an image in numpy format (RGB) and manipulates it. The function should return a numpy array in the same format and be called my_function. The function should ...`\n",
    "\n",
    "Using this prompt and descriptions for what should happen with the image we can manipulate without writing the code ourselves.\n",
    "\n",
    "For example:\n",
    "\n",
    "`The function should make the image grayscale.`\n",
    "\n",
    "`The function should invert the image.`\n",
    "\n",
    "`The function should sort pixels in the image by brightness.`\n",
    "\n",
    "Paste the resulting funtion into your code and execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a3f64a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "\n",
    "# Paste your function here:\n",
    "\n",
    "def my_function(img):\n",
    "    return img\n",
    "\n",
    "img_pil = load_image(\"https://thispersondoesnotexist.com/\")\n",
    "img_np = np.asarray(img_pil)\n",
    "\n",
    "manipulated_img = my_function(img_np)\n",
    "\n",
    "Image.fromarray(manipulated_img).save(\"img/manipulated.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2c030f",
   "metadata": {},
   "source": [
    "## Building a Simple Dynamical System with Image Filters\n",
    "\n",
    "A **dynamical system** evolves over time by repeatedly applying the same (or similar) rules. Here, we’ll apply filters (blur, sharpen, rotate, etc.) in a loop, treating each new image as the input for the next iteration.\n",
    "\n",
    "**Learn more**: [Reaction–diffusion systems](https://en.wikipedia.org/wiki/Reaction–diffusion_system)\n",
    "\n",
    "### Example: Repeated Blur + Sharpen + Rotation\n",
    "We’ll do the following:\n",
    "1. Load an initial image.\n",
    "2. Rotate it slightly, then blur and sharpen.\n",
    "3. Save each iteration.\n",
    "4. Export all frames as an MP4 using `export_to_video`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ad06700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating frames: 100%|██████████| 60/60 [00:04<00:00, 13.70it/s]\n"
     ]
    }
   ],
   "source": [
    "from diffusers.utils import export_to_video\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Define the transformation steps\n",
    "def blur_then_sharpen(input_image: Image.Image) -> Image.Image:\n",
    "    blurred = input_image.filter(ImageFilter.BLUR)\n",
    "    sharpened = blurred.filter(ImageFilter.SHARPEN)\n",
    "    return sharpened\n",
    "\n",
    "# 2. Load a base image\n",
    "original_img = load_image(\"https://thispersondoesnotexist.com/\").convert(\"RGB\")\n",
    "\n",
    "\n",
    "# 3. Iteratively apply transformations\n",
    "num_frames = 60\n",
    "frames = []\n",
    "\n",
    "current_img = original_img\n",
    "angle = 0\n",
    "\n",
    "for i in tqdm(range(num_frames), desc=\"Generating frames\"):\n",
    "    # Rotate slightly\n",
    "    rotated = current_img.rotate(angle, expand=True)\n",
    "\n",
    "    # Blur + Sharpen\n",
    "    processed = rotated.filter(ImageFilter.BLUR).filter(ImageFilter.SHARPEN)\n",
    "\n",
    "    # Resize back to original shape if rotation changed dimensions\n",
    "    processed = processed.resize(original_img.size)\n",
    "\n",
    "    # Accumulate frames\n",
    "    frames.append(processed)\n",
    "\n",
    "    # Prepare next iteration\n",
    "    current_img = processed\n",
    "    angle += 1  # 1 degree each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c19b9b",
   "metadata": {},
   "source": [
    "### Exporting the Frames as an MP4\n",
    "We can now call `export_to_video` to compile these frames into a video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcc870cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is recommended to use `export_to_video` with `imageio` and `imageio-ffmpeg` as a backend. \n",
      "These libraries are not present in your environment. Attempting to use legacy OpenCV backend to export video. \n",
      "Support for the OpenCV backend will be deprecated in a future Diffusers version\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\etien\\AppData\\Local\\Temp\\ipykernel_10700\\4014149615.py\", line 1, in <module>\n",
      "    export_to_video(\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\diffusers\\utils\\export_utils.py\", line 154, in export_to_video\n",
      "    return _legacy_export_to_video(video_frames, output_video_path, fps)\n",
      "  File \"c:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\diffusers\\utils\\export_utils.py\", line 119, in _legacy_export_to_video\n",
      "    import cv2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mexport_to_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_video_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manimation.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\diffusers\\utils\\export_utils.py:154\u001b[0m, in \u001b[0;36mexport_to_video\u001b[1;34m(video_frames, output_video_path, fps)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_imageio_available():\n\u001b[0;32m    147\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    148\u001b[0m         (\n\u001b[0;32m    149\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is recommended to use `export_to_video` with `imageio` and `imageio-ffmpeg` as a backend. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m         )\n\u001b[0;32m    153\u001b[0m     )\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_export_to_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_video_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_imageio_available():\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimageio\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\diffusers\\utils\\export_utils.py:119\u001b[0m, in \u001b[0;36m_legacy_export_to_video\u001b[1;34m(video_frames, output_video_path, fps)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_legacy_export_to_video\u001b[39m(\n\u001b[0;32m    116\u001b[0m     video_frames: Union[List[np\u001b[38;5;241m.\u001b[39mndarray], List[PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage]], output_video_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, fps: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m    117\u001b[0m ):\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_opencv_available():\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(BACKENDS_MAPPING[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopencv\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport_to_video\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "export_to_video(\n",
    "    video_frames=frames,\n",
    "    output_video_path=\"animation.mp4\",\n",
    "    fps=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22526bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting imageio\n",
      "  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages (from imageio) (2.0.2)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\etien\\.conda\\envs\\jupyter-gpu\\lib\\site-packages (from imageio) (11.0.0)\n",
      "Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Installing collected packages: imageio\n",
      "Successfully installed imageio-2.37.0\n"
     ]
    }
   ],
   "source": [
    "!pip install imageio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
